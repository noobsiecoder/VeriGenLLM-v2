# VeriGenLLM-v2 Source Code

This directory contains the core implementation for evaluating and benchmarking Large Language Models (LLMs) on Verilog code generation tasks.

## ðŸ“ Directory Structure

```
src/
â”œâ”€â”€ verigenllm_v2/
â”‚   â”œâ”€â”€ cloud/
â”‚   â”‚   â”œâ”€â”€ gcp_storage.py          # Google Cloud Storage utilities
â”‚   â”‚   â”œâ”€â”€ read_all_questions.py   # Extract prompts from responses
â”‚   â”‚   â””â”€â”€ responses_local_llm.py  # Run open-source LLMs in cloud (local)
â”‚   â”œâ”€â”€ evals/
â”‚   â”‚   â””â”€â”€ pass-k.py               # Pass@k metric evaluation pipeline
â”‚   â”œâ”€â”€ llm_api/
â”‚   â”‚   â”œâ”€â”€ baselines.py            # Run commercial LLMs (Claude, OpenAI)
â”‚   â”‚   â”œâ”€â”€ claude_client.py        # Claude API wrapper
â”‚   â”‚   â”œâ”€â”€ gemini_client.py        # Gemini API wrapper
â”‚   â”‚   â””â”€â”€ openai_client.py        # OpenAI API wrapper
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py               # Centralized logging system
â””â”€â”€ README.md
```

## ðŸš€ Quick Start

### Prerequisites
- Python 3.9+
- [Icarus Verilog](https://steveicarus.github.io/iverilog/) - For compilation checks
- [Yosys](https://yosyshq.net/yosys/) - For synthesis verification
- API keys for commercial LLMs (stored in `secrets/models-api.env`)

### Running Evaluations

1. **Generate responses from commercial LLMs:**
   ```bash
   uv run src/verigenllm_v2/llm_api/baselines.py
   ```

2. **Generate responses from open-source LLMs:**
   ```bash
   uv run src/verigenllm_v2/cloud/responses_local_llm.py
   ```

3. **Run Pass@k evaluation:**
   ```bash
   uv run src/verigenllm_v2/evals/pass_k.py
   ```

## ðŸ”§ Component Overview

### LLM Clients (`llm_api/`)
- **Unified interface** for all LLM providers
- **System prompts** ensure synthesizable Verilog generation
- **Multiple sample generation** (n=10 by default)
- **Response standardization** for consistent evaluation

### Evaluation Pipeline (`evals/pass-k.py`)
Three-stage verification for each generated code:
1. **Compilation** - Icarus Verilog syntax check
2. **Functional Correctness** - Testbench simulation
3. **Synthesizability** - Yosys synthesis check

### Cloud Integration (`cloud/`)
- **GCS Upload** - Automatic backup of results and logs
- **Batch Processing** - Efficient handling of multiple prompts
- **Local LLM Support** - HuggingFace models with GPU acceleration

## ðŸ“Š Evaluation Metrics

**Pass@k**: Measures success rate across k generated samples
- `pass@k-compile`: Proportion that compile successfully
- `pass@k-func-corr`: Proportion passing functional tests  
- `pass@k-sync`: Proportion that synthesize correctly

## ðŸ” Configuration

### API Keys (`secrets/models-api.env`)
```env
CLAUDE_API=your_claude_key
OPENAI_API=your_openai_key
GEMINI_API=your_gemini_key
HUGGINGFACE_TOKEN=your_hf_token
```

### GCS Credentials (`secrets/gcp-storage.json`)
Service account JSON for Google Cloud Storage access

## ðŸ“ Logging

All components use centralized logging:
- **File logs**: `logs/{component}_{YYYY-MM-DD}.log`
- **Console output**: Real-time monitoring
- **Automatic rotation**: Daily log files

## ðŸ§ª Supported Models

### Commercial
- Claude Opus 4
- GPT-4.1
- Gemini 2.5 Pro (not available)

### Open-Source
- CodeLlama-7B-Instruct
- DeepSeek-Coder-7B-Instruct
- Qwen2.5-Coder-7B-Instruct

## ðŸ’¡ Development Tips

1. **Testing single prompts**: Modify the prompt range in baselines.py
2. **Adding new models**: Implement client in `llm_api/` following existing patterns
3. **Custom evaluation**: Extend `PassKMetrics` class in `pass-k.py`
4. **Debugging**: Check daily logs in `logs/` directory

## âš ï¸ Notes

- Gemini API commented out due to issues ith API access
- Claude requires multiple API calls for n>1 samples
- Ensure sufficient GPU memory for local LLMs (~13GB per model)