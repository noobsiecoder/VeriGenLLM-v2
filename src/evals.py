"""
Eval scorer for Verilog code generated by LLMs

Author: Abhishek Sriram <noobsiecoder@gmail.com>
Date:   Aug 16th, 2025
Place:  Boston, MA
"""

import math
import os
import re
import subprocess
import sys
import tempfile
from typing import Dict, List, Tuple
import pandas as pd
from src.logger import Logger


class Evals:
    """
    Benchmark Code generated using Pass@k Metric and other metrics
    This is useful to access each LLM for Verilog code generation
    """

    def __init__(self):
        """
        Loads when object is instantiated
        """
        self.log = Logger("Evals").get_logger()

    def per_question_eval(self, question_responses: List, k: int = 5) -> Dict:
        """
        Generate eval for LLM generated response per question

        Parameters:
        -----------
        question_responses: List
            Data of all the responses per question -> extracted from evals-*.json
        k:                  int
            Used in pass@k metrics (refer _pass_k_metrics method)

        Returns:
        --------
        Dictionary of eval data
        """
        # Pass@k Metric data
        pass_k_metrics = {
            "compilation": float,
            "functional_correctness": float,
            "synthesisability": float,
            "overall": float,  # average over 3
        }
        first_correct_answer_idx = (
            -1
        )  # Used to determine the idx of the first correct code generated by the LLM

        # Eval Pass@k Metric
        n = len(question_responses["evals"])
        c_compile = 0
        c_func = 0
        c_synth = 0
        # print(question_responses["evals"])
        for idx, question_response in enumerate(question_responses["evals"]):
            if question_response["compilation"]["status"]:
                c_compile += 1
            if question_response["functional_correctness"]["status"]:
                c_func += 1
            if question_response["synthesisability"]["status"]:
                c_synth += 1
            # Change index only if all are correct
            # Will occur only once if match found
            if (
                question_response["compilation"]["status"]
                and question_response["functional_correctness"]["status"]
                and question_response["synthesisability"]["status"]
                and first_correct_answer_idx == -1
            ):
                first_correct_answer_idx = idx
        # Pass@k Metric data: COMPILE
        pass_k_metrics["compilation"] = self._pass_k_metric(n, c_compile, k)
        # Pass@k Metric data: FUNCTIONAL CORRECTNESS
        pass_k_metrics["functional_correctness"] = self._pass_k_metric(n, c_func, k)
        # Pass@k Metric data: SYNTHESISABILITY
        pass_k_metrics["synthesisability"] = self._pass_k_metric(n, c_synth, k)
        # Pass@k Metric data: OVERALL
        pass_k_metrics["overall"] = (
            pass_k_metrics["compilation"]
            + pass_k_metrics["functional_correctness"]
            + pass_k_metrics["synthesisability"]
        ) / 3

        # Get number of unique correct answers
        correct_codes = []
        for question_response in question_responses["evals"]:
            if (
                question_response["compilation"]["status"]
                and question_response["functional_correctness"]["status"]
                and question_response["synthesisability"]["status"]
            ):
                correct_codes.append(
                    question_response["code_analysis"]["code"]["output"]
                )
        unique_codes = set(correct_codes)
        unique_code_len = len(unique_codes)

        return {
            "pass_k_metric": pass_k_metrics,
            "first_correct_idx": first_correct_answer_idx,
            "total_outputs": len(question_responses["evals"]),
            "correct_codes_len": len(correct_codes),
            "unique_code_len": unique_code_len,
            "k": k,
        }

    def per_model_eval(self, model_df: pd.DataFrame) -> Dict:
        """
        Generate eval for LLM generated response per question

        Parameters:
        -----------
        model_df: pd.DataFrame
            Data of all the responses per model -> extracted from metrics-*.csv

        Returns:
        --------
        Dictionary of eval data
        """
        # Pass@k Metric data

    def _pass_k_metric(self, n: int, c: int, k=int) -> float:
        """
        From a blog found in https://www.datacamp.com/:
        "Pass@k metric to evaluate the probability that at least one of the top k-generated code samples for a problem passes the unit tests."

        Pass@k Metric = 1 - [C(n - c, k) / C(n, k)]

        Parameters:
        -----------
        n: int
            Total number of samples generated
        c: int
            Correct samples among 'n' samples
        k: int
            Number of samples considered (top k)

        Returns:
        --------
        Float value representing the probability of at least one of the 'k' samples is correct
        """
        return 1 - (math.comb(n - c, k) / math.comb(n, k))


class ResponseEvals:
    """
    Evaluates generated response from the LLM
    """

    def __init__(self):
        """
        Loads when object is instantiated
        """
        self.log = Logger("ResponseEvals").get_logger()

    def evaluate_response(
        self,
        response: Dict,
        analyse_code: bool = True,
        analyse_compilation: bool = True,
        analyse_functional_correctness: bool = True,
        analyse_synthesisability: bool = True,
    ) -> Dict:
        """
        Get response's eval data on its code generation

        Parameters:
        -----------
        response:                       Dict
            Data retreived from LLM (generated response)
        analyse_code:                   bool,True
            Flag to analyse code
        analyse_compilation:            bool, True
            Flag to analyse compilation of generated code
        analyse_functional_correctness: bool, True
            Flag to analyse functional correctness of generated code
        analyse_synthesisability:       bool, True
            Flag to analyse synthesisability of generated code

        Returns:
        --------
        Dict of the response eval

        Note:
        -----
        This only takes one sample from the responses (and not 'n' samples of a response)
        """
        response_eval = {}
        meta = {
            "model": response["model"],
            "temperature": response["temperature"],
            "max_tokens_allowed": response["max_tokens"],
            # "tokens": {
            #     "input": int,
            #     "output": int,
            #     "total": int,
            # },
            # "latency": float,
        }

        response_eval["meta"] = meta
        if analyse_code:
            code_analysis = self._analyse_code(response["output"])
            response_eval["code_analysis"] = code_analysis

        if analyse_compilation:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            compilation = self._analyse_compilation(code_str)
            response_eval["compilation"] = compilation

        if analyse_functional_correctness:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            functional_correctness = self._analyse_functional_correctness(
                code_str, testbench_code_str=response["testbench"]
            )
            response_eval["functional_correctness"] = functional_correctness

        if analyse_synthesisability:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            synthesisability = self._analyse_synthesisability(code_str)
            response_eval["synthesisability"] = synthesisability

        return response_eval

    def _analyse_code(self, output: str) -> Dict:
        """
        Analyse generated code quality and other available metrics from the code

        Parameters:
        -----------
        output: str
            Response generated by the LLM

        Returns:
        --------
        Dictionary of code - analysed data
        """
        code_analysis = {
            # "modules": {
            #     "names": [str],
            #     "count": int,
            # },
            "code": {
                "attempted": bool,
                "comments": int,
                "completed": bool,
                "lines": int,
                "output": str,
            },
            "misc": {
                "md_style": bool,
            },
        }

        pattern = r"(module.*\nendmodule)"
        match = re.search(pattern, output, re.DOTALL)

        if match:
            # match found -> LLM attempted to generate code
            code_analysis["code"]["attempted"] = True
            # match for includes both module...endmodule
            code_analysis["code"]["completed"] = True
            # get code
            code_analysis["code"]["output"] = match.group(1)
            # get total lines
            newline_pattern = r"\n"
            expression = re.compile(newline_pattern)
            code_analysis["code"]["lines"] = (
                len(expression.findall(code_analysis["code"]["output"])) + 1
            )
            # get total comments in code
            comments_pattern = r"\/\/"
            expression = re.compile(comments_pattern)
            code_analysis["code"]["comments"] = len(
                expression.findall(code_analysis["code"]["output"])
            )
            # Check if MD style is followed
            md_pattern = r"```verilog(.*)```"
            new_match = re.search(md_pattern, output, re.DOTALL)
            if new_match:
                if (
                    code_analysis["code"]["output"].strip()
                    == new_match.group(1).strip()
                ):
                    code_analysis["misc"]["md_style"] = True
            else:
                code_analysis["misc"]["md_style"] = False
        else:
            # match not found -> LLM may have attempted but unsure
            code_analysis["code"]["completed"] = False
            # Check if LLM attempted to generate code
            mod_pattern = r"module (.*)"
            match = re.search(mod_pattern, output, re.DOTALL)
            if match:
                code_analysis["code"]["attempted"] = True
                # get code
                code_analysis["code"]["output"] = match.group(1)
                # get total lines
                newline_pattern = r"\n"
                expression = re.compile(newline_pattern)
                code_analysis["code"]["lines"] = (
                    len(expression.findall(code_analysis["code"]["output"])) + 1
                )
                # get total comments in code
                comments_pattern = r"\/\/"
                expression = re.compile(comments_pattern)
                code_analysis["code"]["comments"] = len(
                    expression.findall(code_analysis["code"]["output"])
                )
                # Check if MD style is followed
                md_pattern = r"```verilog(.*)```"
                new_match = re.search(md_pattern, output, re.DOTALL)
                if new_match:
                    if (
                        code_analysis["code"]["output"].strip()
                        == new_match.group(1).strip()
                    ):
                        code_analysis["misc"]["md_style"] = True
                else:
                    code_analysis["misc"]["md_style"] = False
            else:
                code_analysis["code"]["attempted"] = False

        # Chances of some fields being empty is possible
        # Handle edge case
        if code_analysis["code"]["comments"] == int:
            code_analysis["code"]["comments"] = 0
        if code_analysis["code"]["lines"] == int:
            code_analysis["code"]["lines"] = 0
        if code_analysis["code"]["output"] == str:
            code_analysis["code"]["output"] = None
        if code_analysis["misc"]["md_style"] == bool:
            code_analysis["misc"]["md_style"] = False

        return code_analysis

    def _group_errors_and_warnings(
        self, std_errors: List[str]
    ) -> Tuple[List[str], List[str]]:
        """
        Group Errors and Warnings from Compilation, Func-corr, and Synth..

        Parameters:
        -----------
        std_errors: List[str]
            Errors + Warnings -> captured from console output (in silent mode)

        Returns:
        --------
        Tuple of both Errors and Warnings
        """
        warnings = []
        w_idx = -1
        errors = []
        idx = 0
        while idx < len(std_errors):
            if "warning" in std_errors[idx]:
                warnings.append(std_errors[idx])
                w_idx += 1
            elif "error" in std_errors[idx]:
                idx += 1  # increment index before breaking from loop
                break
            else:
                if w_idx >= 0:
                    warnings[w_idx] += std_errors[idx]
            idx += 1  # increment index -> important
        for i in range(idx, len(std_errors)):
            errors.append(std_errors[i])

        errors = errors if len(errors) != 0 else None
        warnings = warnings if len(warnings) != 0 else None
        return (errors, warnings)

    def _analyse_compilation(self, code_str: str) -> Dict:
        """
        Analyse compilation status of the code from response
        Runs iverilog and vvp w/o testbench

        Parameters:
        -----------
        code_str: str
            Code extracted from LLM's response

        Returns:
        --------
        Dictionary of compilation - analysed data
        """
        compilation = {
            "status": bool,
            "error": {
                "count": int,
                "types": [str],
            },
            "warning": {
                "count": int,
                "types": [str],
            },
        }

        self.log.info("Inside Compilation")
        # If code wasn't generated, prepare response and return early
        if code_str == None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            compilation["status"] = False
            # warnings
            compilation["warning"]["types"] = None
            compilation["warning"]["count"] = None
            # errors
            compilation["error"]["types"] = None
            compilation["error"]["count"] = None

            return compilation

        temp_sol_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(code_str)
            temp_sol_filepath = temp_file.name

        result = subprocess.run(
            [
                "iverilog",
                "-Wall",
                "-Winfloop",  # Warn about infinite loops
                "-Wanachronisms",  # Warn about old constructs
                "-Wsensitivity-entire-array",  # Array sensitivity warnings
                "-o",
                "test_output",  # Output binary name
                temp_sol_filepath,
            ],
            capture_output=True,  # Capture stdout and stderr
            text=True,  # Return output as string
        )
        os.remove(temp_sol_filepath)  # remove temp file from local machine
        if os.path.exists("test_output"):
            os.remove("test_output")  # remove output file from local machine

        compilation["status"] = result.returncode == 0
        if compilation["status"]:
            self.log.info("✓ Compilation: PASS")
        else:
            self.log.error("✗ Compilation: FAIL")

        # Check for errors and warnings:
        if result.stderr:
            self.log.warning("⚠ Warning/Error found")
            std_errors = result.stderr.splitlines()
            errors, warnings = self._group_errors_and_warnings(std_errors)
            # Store errors and warnings
            compilation["error"]["types"] = errors
            compilation["error"]["count"] = None if errors == None else len(errors)
            compilation["warning"]["types"] = warnings
            compilation["warning"]["count"] = (
                None if warnings == None else len(warnings)
            )
        else:
            self.log.warning("No Warning/Error found")
            # Store errors and warnings for success
            compilation["error"]["types"] = None
            compilation["error"]["count"] = None
            compilation["warning"]["types"] = None
            compilation["warning"]["count"] = None

        return compilation

    def _analyse_functional_correctness(
        self, code_str: str, testbench_code_str: str
    ) -> Dict:
        """
        Analyse functional correctness status of the generated code from response
        Runs iverilog and vvp with testbench

        Parameters:
        -----------
        code_str:           str
            Code extracted from LLM's response
        testbench_code_str: str
            Testbench code from VeriGen Repository

        Returns:
        --------
        Dictionary of functional correctness - analysed data
        """
        functional_correctness = {
            "status": bool,
            # "test_cases": {
            #     "total": int,
            #     "pass": int,
            # },
            "error": {
                "count": int,
                "types": [str],
            },
            "warning": {
                "count": int,
                "types": [str],
            },
        }

        self.log.info("Inside Functional Correctness")
        # If code wasn't generated, prepare response and return early
        if code_str == None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            functional_correctness["status"] = False
            # test cases
            functional_correctness["error"]["types"] = None
            functional_correctness["error"]["count"] = None
            # errors
            functional_correctness["warning"]["types"] = None
            functional_correctness["warning"]["count"] = None

            return functional_correctness

        temp_sol_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(code_str)
            temp_sol_filepath = temp_file.name

        temp_tb_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(testbench_code_str)
            temp_tb_filepath = temp_file.name

        result = subprocess.run(
            [
                "iverilog",
                "-Wall",
                "-Winfloop",  # Warn about infinite loops
                "-Wanachronisms",  # Warn about old constructs
                "-Wsensitivity-entire-array",  # Array sensitivity warnings
                "-o",
                "test_output",  # Output binary name
                temp_sol_filepath,
                temp_tb_filepath,
            ],
            capture_output=True,  # Capture stdout and stderr
            text=True,  # Return output as string
        )

        os.remove(temp_sol_filepath)  # remove temp file from local machine
        os.remove(temp_tb_filepath)  # remove temp file from local machine
        # If not compiled successfully
        if result.returncode != 0:
            self.log.error("✗ Compilation -> Functional Correctness: FAIL")
            functional_correctness["status"] = False

        # Check for errors and warnings:
        if result.stderr:
            self.log.error("Inside Errors and Warnings checker after compilation in FC")
            std_errors = result.stderr.splitlines()
            errors, warnings = self._group_errors_and_warnings(std_errors)
            # Store errors and warnings
            functional_correctness["error"]["types"] = errors
            functional_correctness["error"]["count"] = (
                None if errors == None else len(errors)
            )
            functional_correctness["warning"]["types"] = warnings
            functional_correctness["warning"]["count"] = (
                None if warnings == None else len(warnings)
            )
            if result.returncode != 0:
                return functional_correctness

        # If compiled successfully
        func_result = subprocess.run(
            ["vvp", "test_output"],
            capture_output=True,  # Capture stdout and stderr
            text=True,  # Return output as string
        )
        os.remove("test_output")  # remove output file from local machine
        # Check if tests passed
        stdout = func_result.stdout
        if "all tests passed" in stdout:
            self.log.error("✓ Functional Correctness: PASS")
            functional_correctness["status"] = True
            # Store errors and warnings
            functional_correctness["error"]["types"] = None
            functional_correctness["error"]["count"] = None
            functional_correctness["warning"]["types"] = None
            functional_correctness["warning"]["count"] = None
        else:
            self.log.error("✗ Compilation -> Functional Correctness: FAIL")
            functional_correctness["status"] = False
            fail_pattern = r"(test.*failed)"
            expression = re.compile(fail_pattern)
            errors = expression.findall(stdout)

            # Store errors (checker to ensure previous warnings aren't erased)
            if functional_correctness["error"]["types"] == [str]:
                functional_correctness["error"]["types"].append(errors)
            else:
                functional_correctness["error"]["types"] = errors
            functional_correctness["error"]["count"] = len(
                errors
            )  # no checker required
            # Store warnings (checker to ensure previous warnings aren't erased)
            if functional_correctness["warning"]["types"] == int:
                functional_correctness["warning"]["types"] = None
            if functional_correctness["error"]["types"] == int:
                functional_correctness["warning"]["count"] = None

        return functional_correctness

    def _analyse_synthesisability(self, code_str: str) -> Dict:
        """
        Analyse compilation status of the generated code from response
        Runs Yosys for checking synthesisability

        Parameters:
        -----------
        code_str:           str
            Code extracted from LLM's response

        Returns:
        --------
        Dictionary of functional correctness - analysed data
        """
        synthesisability = {
            "status": bool,
            # "output": str,
            "tool": str,
            "error": {
                "count": int,
                "types": [str],
            },
            "warning": {
                "count": int,
                "types": [str],
            },
        }

        self.log.info("Inside Synthesisability")
        # Get tool data
        tool_result = subprocess.run(
            ["yosys", "-V"],
            capture_output=True,  # Capture stdout and stderr
            text=True,  # Return output as string
        )
        if tool_result.stdout:
            synthesisability["tool"] = tool_result.stdout
        else:
            self.log.critical("✗ Synthesisability tool not found (probably Yosys)")
            self.log.critical("Exiting script...")
            sys.exit(-1)

        # If code wasn't generated
        if code_str == None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            synthesisability["status"] = False
            synthesisability["error"]["types"] = None
            synthesisability["error"]["count"] = None
            synthesisability["warning"]["types"] = None
            synthesisability["warning"]["count"] = None

            return synthesisability

        temp_sol_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(code_str)
            temp_sol_filepath = temp_file.name

        # Check code synthesisability
        result = subprocess.run(
            [
                "yosys",
                "-q",
                "-p",
                f"read_verilog {temp_sol_filepath}; "
                "hierarchy -check; "
                "proc; opt; "
                "synth; "
                "check; "
                "write_verilog /dev/null",
            ],
            capture_output=True,
            text=True,
        )

        os.remove(temp_sol_filepath)  # remove output file from local machine

        # Check if synthesisable
        synthesisability["status"] = result.returncode == 0
        if synthesisability["status"]:
            self.log.info("✓ Synthesisability: PASS")
        else:
            self.log.info("✗ Synthesisability: FAIL")
        if result.stderr:
            self.log.warning("⚠ Warning/Error found")
            std_errors = result.stderr.splitlines()
            # Check for errors and warnings:
            errors, warnings = self._group_errors_and_warnings(std_errors)
            synthesisability["error"]["types"] = errors
            synthesisability["error"]["count"] = None if errors == None else len(errors)
            synthesisability["warning"]["types"] = warnings
            synthesisability["warning"]["count"] = (
                None if warnings == None else len(warnings)
            )
        else:
            synthesisability["error"]["types"] = None
            synthesisability["error"]["count"] = None
            synthesisability["warning"]["types"] = None
            synthesisability["warning"]["count"] = None

        return synthesisability
