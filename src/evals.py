"""
Eval scorer for Verilog code generated by LLMs

Author: Abhishek Sriram <noobsiecoder@gmail.com>
Date:   Aug 16th, 2025
Place:  Boston, MA
"""

import math
import os
import re
import subprocess
import sys
import tempfile
from typing import Dict, List, Tuple
import pandas as pd
from src.logger import Logger


class Evals:
    """
    Benchmark Code generated using Pass@k Metric and other metrics
    This is useful to access each LLM for Verilog code generation

    TODO: This class is the most buggiest code I have written - thanks to Python.
        - Yet somehow, it works. (like an ugly glue)
        - Before it becomes a *Tech Debt*, clean it up!
    NOTE: Use this class with caution
    """

    def __init__(self):
        """
        Loads when object is instantiated
        """
        self.log = Logger("Evals").get_logger()

    def per_question_eval(self, question_responses: List, k: int = 5) -> Dict:
        """
        Generate eval for LLM generated response per question

        Parameters:
        -----------
        question_responses: List
            Data of all the responses per question -> extracted from evals-*.json
        k:                  int
            Used in pass@k metrics (refer _pass_k_metrics method)

        Returns:
        --------
        Dictionary of eval data
        """
        # Pass@k Metric data
        pass_k_metrics = {
            "compilation": float,
            "functional_correctness": float,
            "synthesisability": float,
            "overall": float,  # average over 3
        }
        first_correct_answer_idx = (
            -1
        )  # Used to determine the idx of the first correct code generated by the LLM

        # Eval Pass@k Metric
        n = len(question_responses["evals"])
        c_compile = 0
        c_func = 0
        c_synth = 0
        # print(question_responses["evals"])
        for idx, question_response in enumerate(question_responses["evals"]):
            if question_response["compilation"]["status"]:
                c_compile += 1
            if question_response["functional_correctness"]["status"]:
                c_func += 1
            if question_response["synthesisability"]["status"]:
                c_synth += 1
            # Change index only if all are correct
            # Will occur only once if match found
            if (
                question_response["compilation"]["status"]
                and question_response["functional_correctness"]["status"]
                and question_response["synthesisability"]["status"]
                and first_correct_answer_idx == -1
            ):
                first_correct_answer_idx = idx
        # Pass@k Metric data: COMPILE
        pass_k_metrics["compilation"] = self._pass_k_metric(n, c_compile, k)
        # Pass@k Metric data: FUNCTIONAL CORRECTNESS
        pass_k_metrics["functional_correctness"] = self._pass_k_metric(n, c_func, k)
        # Pass@k Metric data: SYNTHESISABILITY
        pass_k_metrics["synthesisability"] = self._pass_k_metric(n, c_synth, k)
        # Pass@k Metric data: OVERALL
        pass_k_metrics["overall"] = (
            pass_k_metrics["compilation"]
            + pass_k_metrics["functional_correctness"]
            + pass_k_metrics["synthesisability"]
        ) / 3

        # Get number of unique correct answers
        correct_codes = []
        for question_response in question_responses["evals"]:
            if (
                question_response["compilation"]["status"]
                and question_response["functional_correctness"]["status"]
                and question_response["synthesisability"]["status"]
            ):
                correct_codes.append(
                    question_response["code_analysis"]["code"]["output"]
                )
        unique_codes = set(correct_codes)
        unique_code_len = len(unique_codes)

        return {
            "pass_k_metric": pass_k_metrics,
            "first_correct_idx": first_correct_answer_idx,
            "total_outputs": len(question_responses["evals"]),
            "correct_codes_len": len(correct_codes),
            "unique_code_len": unique_code_len,
            "k": k,
        }

    def per_model_eval(self, model_df: pd.DataFrame) -> Dict:
        """
        Generate eval for LLM generated response per question

        Parameters:
        -----------
        model_df: pd.DataFrame
            Data of all the responses per model -> extracted from metrics-*.csv

        Returns:
        --------
        Dictionary of eval data
        """
        # Pass@k Metric data

    def _pass_k_metric(self, n: int, c: int, k=int) -> float:
        """
        From a blog found in https://www.datacamp.com/:
        "Pass@k metric to evaluate the probability that at least one of the top k-generated code samples for a problem passes the unit tests."

        Pass@k Metric = 1 - [C(n - c, k) / C(n, k)]

        Parameters:
        -----------
        n: int
            Total number of samples generated
        c: int
            Correct samples among 'n' samples
        k: int
            Number of samples considered (top k)

        Returns:
        --------
        Float value representing the probability of at least one of the 'k' samples is correct
        """
        return 1 - (math.comb(n - c, k) / math.comb(n, k))


class ResponseEvals:
    """
    Evaluates generated response from the LLM
    """

    def __init__(self):
        """
        Loads when object is instantiated
        """
        self.log = Logger("ResponseEvals").get_logger()

    def evaluate_response(
        self,
        response: Dict,
        analyse_code: bool = True,
        analyse_compilation: bool = True,
        analyse_functional_correctness: bool = True,
        analyse_synthesisability: bool = True,
    ) -> Dict:
        """
        Get response's eval data on its code generation

        Parameters:
        -----------
        response:                       Dict
            Data retreived from LLM (generated response)
        analyse_code:                   bool,True
            Flag to analyse code
        analyse_compilation:            bool, True
            Flag to analyse compilation of generated code
        analyse_functional_correctness: bool, True
            Flag to analyse functional correctness of generated code
        analyse_synthesisability:       bool, True
            Flag to analyse synthesisability of generated code

        Returns:
        --------
        Dict of the response eval

        Note:
        -----
        This only takes one sample from the responses (and not 'n' samples of a response)
        """
        response_eval = {}
        meta = {
            "model": response["model"],
            "temperature": response["temperature"],
            "max_tokens_allowed": response["max_tokens"],
            # "tokens": {
            #     "input": int,
            #     "output": int,
            #     "total": int,
            # },
            # "latency": float,
        }

        response_eval["meta"] = meta
        if analyse_code:
            code_analysis = self._analyse_code(response["output"])
            response_eval["code_analysis"] = code_analysis

        if analyse_compilation:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            compilation = self._analyse_compilation(code_str)
            response_eval["compilation"] = compilation

        if analyse_functional_correctness:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            functional_correctness = self._analyse_functional_correctness(
                code_str, testbench_code_str=response["testbench"]
            )
            response_eval["functional_correctness"] = functional_correctness

        if analyse_synthesisability:
            code_str = None
            if response_eval["code_analysis"]["code"]["completed"]:
                code_str = response_eval["code_analysis"]["code"]["output"]
            synthesisability = self._analyse_synthesisability(code_str)
            response_eval["synthesisability"] = synthesisability

        return response_eval

    def _analyse_code(self, output: str) -> Dict:
        """
        Analyse generated code quality and other available metrics from the code

        Parameters:
        -----------
        output: str
            Response generated by the LLM

        Returns:
        --------
        Dictionary of code - analysed data
        """
        code_analysis = {
            # "modules": {
            #     "names": [str],
            #     "count": int,
            # },
            "code": {
                "attempted": None,
                "comments": None,
                "completed": None,
                "lines": None,
                "output": None,
            },
            "misc": {
                "md_style": None,
            },
        }

        pattern = r"(module.*endmodule)"
        match = re.search(pattern, output, re.DOTALL)

        if match:
            # match found -> LLM attempted to generate code
            code_analysis["code"]["attempted"] = True
            # match for includes both module...endmodule
            code_analysis["code"]["completed"] = True
            # get code
            code_analysis["code"]["output"] = match.group(1)
            # get total lines
            newline_pattern = r"\n"
            expression = re.compile(newline_pattern)
            code_analysis["code"]["lines"] = (
                len(expression.findall(code_analysis["code"]["output"])) + 1
            )
            # get total comments in code
            comments_pattern = r"\/\/"
            expression = re.compile(comments_pattern)
            code_analysis["code"]["comments"] = len(
                expression.findall(code_analysis["code"]["output"])
            )
            # Check if MD style is followed
            md_pattern = r"```verilog(.*)```"
            new_match = re.search(md_pattern, output, re.DOTALL)
            if new_match:
                if (
                    code_analysis["code"]["output"].strip()
                    == new_match.group(1).strip()
                ):
                    code_analysis["misc"]["md_style"] = True
            else:
                code_analysis["misc"]["md_style"] = False
        else:
            # match not found -> LLM may have attempted but unsure
            code_analysis["code"]["completed"] = False
            # Check if LLM attempted to generate code
            mod_pattern = r"module (.*)"
            match = re.search(mod_pattern, output, re.DOTALL)
            if match:
                code_analysis["code"]["attempted"] = True
                # get code
                code_analysis["code"]["output"] = match.group(1)
                # get total lines
                newline_pattern = r"\n"
                expression = re.compile(newline_pattern)
                code_analysis["code"]["lines"] = (
                    len(expression.findall(code_analysis["code"]["output"])) + 1
                )
                # get total comments in code
                comments_pattern = r"\/\/"
                expression = re.compile(comments_pattern)
                code_analysis["code"]["comments"] = len(
                    expression.findall(code_analysis["code"]["output"])
                )
                # Check if MD style is followed
                md_pattern = r"```verilog(.*)```"
                new_match = re.search(md_pattern, output, re.DOTALL)
                if new_match:
                    if (
                        code_analysis["code"]["output"].strip()
                        == new_match.group(1).strip()
                    ):
                        code_analysis["misc"]["md_style"] = True
                else:
                    code_analysis["misc"]["md_style"] = False
            else:
                code_analysis["code"]["attempted"] = False

        # Chances of some fields being empty is possible
        # Handle edge case
        if code_analysis["code"]["comments"] is None:
            code_analysis["code"]["comments"] = 0
        if code_analysis["code"]["lines"] is None:
            code_analysis["code"]["lines"] = 0
        if code_analysis["code"]["output"] is None:
            code_analysis["code"]["output"] = None
        if code_analysis["misc"]["md_style"] is None:
            code_analysis["misc"]["md_style"] = False

        return code_analysis

    def _run_subprocess(
        self, commands: List[str], timeout_ms: int = 500
    ) -> Tuple[bool, subprocess.CompletedProcess[str]]:
        """
        Run subprocess and kill thread if overruns

        Parameters:
        -----------
        commands: List[str]
            commands to run via terminal
        timeout_ms: int
            Time to wait until thread is killed

        Returns:
        --------
        Bool to indicate status and result in subprocess.CompletedProcess object
        On Failure -> returns (False, None)
        """
        try:
            func_result = subprocess.run(
                commands,
                capture_output=True,
                text=True,
                timeout=timeout_ms / 1000.0,  # convert ms → seconds
            )
            # return 1 for success if returncode == 0
            return (
                (True, func_result)
                if func_result.returncode == 0
                else (False, func_result)
            )
        except subprocess.TimeoutExpired:
            # killed after timeout
            return (False, None)
        except Exception as err:
            self.log.warning(
                f"While running {' '.join(commands)}\nError ocurred: {err}"
            )
            # some other failure
            return (False, None)

    def _group_errors_and_warnings(
        self, std_errors: List[str]
    ) -> Tuple[List[str], List[str]]:
        """
        Group Errors and Warnings from Compilation, Func-corr, and Synth..
        """
        warnings = []
        w_idx = -1
        errors = []
        self.log.info(f"ERROR MSG: {std_errors}")

        # Process warnings until the first error is found
        for idx, line in enumerate(std_errors):
            if "warning" in line:
                warnings.append(line)
                w_idx += 1
            elif "error" in line:
                # move past the error line (like in your while loop)
                start_idx = idx + 1
                break
            else:
                if w_idx >= 0:
                    warnings[w_idx] += line
        else:
            # if no 'error' line is found, start_idx should equal len(std_errors)
            start_idx = len(std_errors)

        self.log.info("Left for loop")

        # Collect remaining lines as errors
        for i in range(start_idx, len(std_errors)):
            errors.append(std_errors[i])

        errors = errors if errors else None
        warnings = warnings if warnings else None
        return errors, warnings

    def _analyse_compilation(self, code_str: str) -> Dict:
        """
        Analyse compilation status of the code from response
        Runs iverilog and vvp w/o testbench

        Parameters:
        -----------
        code_str: str
            Code extracted from LLM's response

        Returns:
        --------
        Dictionary of compilation - analysed data
        """
        compilation = {
            "status": None,
            "error": {
                "count": None,
                "types": None,
            },
            "warning": {
                "count": None,
                "types": None,
            },
        }

        self.log.info("Inside Compilation")
        # If code wasn't generated, prepare response and return early
        if code_str is None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            compilation["status"] = False
            # warnings
            compilation["warning"]["types"] = None
            compilation["warning"]["count"] = None
            # errors
            compilation["error"]["types"] = None
            compilation["error"]["count"] = None

            return compilation

        temp_sol_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(code_str)
            temp_sol_filepath = temp_file.name

        commands = [
            "iverilog",
            "-Wall",
            "-Winfloop",  # Warn about infinite loops
            "-Wanachronisms",  # Warn about old constructs
            "-Wsensitivity-entire-array",  # Array sensitivity warnings
            "-o",
            "test_output",  # Output binary name
            temp_sol_filepath,
        ]
        _, result = self._run_subprocess(commands)
        os.remove(temp_sol_filepath)  # remove temp file from local machine
        if os.path.exists("test_output"):
            os.remove("test_output")  # remove output file from local machine

        compilation["status"] = result.returncode == 0
        if compilation["status"]:
            self.log.info("✓ Compilation: PASS")
        else:
            self.log.error("✗ Compilation: FAIL")

        # Check for errors and warnings:
        if result.stderr:
            self.log.warning("⚠ Warning/Error found")
            std_errors = result.stderr.splitlines()
            errors, warnings = self._group_errors_and_warnings(std_errors)
            # Store errors and warnings
            compilation["error"]["types"] = errors
            compilation["error"]["count"] = None if errors is None else len(errors)
            compilation["warning"]["types"] = warnings
            compilation["warning"]["count"] = (
                None if warnings is None else len(warnings)
            )
        else:
            self.log.warning("No Warning/Error found")
            # Store errors and warnings for success
            compilation["error"]["types"] = None
            compilation["error"]["count"] = None
            compilation["warning"]["types"] = None
            compilation["warning"]["count"] = None

        return compilation

    def _analyse_functional_correctness(
        self, code_str: str, testbench_code_str: str
    ) -> Dict:
        """
        Analyse functional correctness status of the generated code from response
        Runs iverilog and vvp with testbench

        Parameters:
        -----------
        code_str:           str
            Code extracted from LLM's response
        testbench_code_str: str
            Testbench code from VeriGen Repository

        Returns:
        --------
        Dictionary of functional correctness - analysed data
        """
        functional_correctness = {
            "status": None,
            "error": {"count": None, "types": None},
            "warning": {"count": None, "types": None},
        }

        self.log.info("Inside Functional Correctness")

        # If code wasn't generated, prepare response and return early
        if code_str is None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            functional_correctness["status"] = False
            return functional_correctness

        # Write solution and testbench to temp files
        with tempfile.NamedTemporaryFile(
            suffix=".v", mode="w+", delete=False
        ) as sol_file:
            sol_file.write(code_str)
            temp_sol_filepath = sol_file.name

        with tempfile.NamedTemporaryFile(
            suffix=".v", mode="w+", delete=False
        ) as tb_file:
            tb_file.write(testbench_code_str)
            temp_tb_filepath = tb_file.name

        # Compile with iverilog
        commands = [
            "iverilog",
            "-Wall",
            "-Winfloop",
            "-Wanachronisms",
            "-Wsensitivity-entire-array",
            "-o",
            "test_output",
            temp_sol_filepath,
            temp_tb_filepath,
        ]
        _, result = self._run_subprocess(commands)

        # Clean up temporary files
        os.remove(temp_sol_filepath)
        os.remove(temp_tb_filepath)

        if result.returncode != 0:
            self.log.error("✗ Compilation -> Functional Correctness: FAIL")
            functional_correctness["status"] = False

        # Parse errors and warnings from stderr
        if result.stderr:
            std_errors = result.stderr.splitlines()
            errors, warnings = self._group_errors_and_warnings(std_errors)

            functional_correctness["error"]["types"] = errors if errors else None
            functional_correctness["error"]["count"] = len(errors) if errors else None
            functional_correctness["warning"]["types"] = warnings if warnings else None
            functional_correctness["warning"]["count"] = (
                len(warnings) if warnings else None
            )

            if result.returncode != 0:
                return functional_correctness

        # Run vvp if compilation succeeded
        _, func_result = self._run_subprocess(["vvp", "test_output"])
        os.remove("test_output")

        if func_result is None:
            self.log.error("✗ Functional Correctness: FAIL")
            self.log.error("Time exceeded")
            functional_correctness["status"] = False
            functional_correctness["error"]["types"] = ["Time Exceeded"]
            functional_correctness["error"]["count"] = 1
            functional_correctness["warning"]["types"] = None
            functional_correctness["warning"]["count"] = None
            return functional_correctness  # Early return block

        stdout = func_result.stdout
        if "all tests passed" in stdout:
            self.log.info("✓ Functional Correctness: PASS")
            functional_correctness["status"] = True
            functional_correctness["error"]["types"] = None
            functional_correctness["error"]["count"] = None
            functional_correctness["warning"]["types"] = None
            functional_correctness["warning"]["count"] = None
        else:
            self.log.error("✗ Functional Correctness: FAIL")
            functional_correctness["status"] = False

            # Extract failing tests
            fail_pattern = r"(test.*failed)"
            errors = re.findall(fail_pattern, stdout)

            functional_correctness["error"]["types"] = errors if errors else None
            functional_correctness["error"]["count"] = len(errors) if errors else None
            # Leave warnings as they were parsed above (don’t overwrite unless None)

        return functional_correctness

    def _analyse_synthesisability(self, code_str: str) -> Dict:
        """
        Analyse compilation status of the generated code from response
        Runs Yosys for checking synthesisability

        Parameters:
        -----------
        code_str:           str
            Code extracted from LLM's response

        Returns:
        --------
        Dictionary of functional correctness - analysed data
        """
        synthesisability = {
            "status": None,
            # "output": str,
            "tool": None,
            "error": {
                "count": None,
                "types": None,
            },
            "warning": {
                "count": None,
                "types": None,
            },
        }

        self.log.info("Inside Synthesisability")
        # Get tool data
        tool_result = subprocess.run(
            ["yosys", "-V"],
            capture_output=True,  # Capture stdout and stderr
            text=True,  # Return output as string
        )
        if tool_result.stdout:
            synthesisability["tool"] = tool_result.stdout
        else:
            self.log.critical("✗ Synthesisability tool not found (probably Yosys)")
            self.log.critical("Exiting script...")
            sys.exit(-1)

        # If code wasn't generated
        if code_str is None:
            self.log.warning("⚠ Code wasn't generated, skipping...")
            synthesisability["status"] = False
            synthesisability["error"]["types"] = None
            synthesisability["error"]["count"] = None
            synthesisability["warning"]["types"] = None
            synthesisability["warning"]["count"] = None

            return synthesisability

        temp_sol_filepath = None
        with tempfile.NamedTemporaryFile(
            suffix=".v",
            mode="w+",
            delete=False,  # Keep file after closing
        ) as temp_file:
            temp_file.write(code_str)
            temp_sol_filepath = temp_file.name

        # Check code synthesisability
        commands = [
            "yosys",
            "-q",
            "-p",
            f"read_verilog {temp_sol_filepath}; "
            "hierarchy -check; "
            "proc; opt; "
            "synth; "
            "check; "
            "write_verilog /dev/null",
        ]
        _, result = self._run_subprocess(commands)

        os.remove(temp_sol_filepath)  # remove output file from local machine

        # Check if synthesisable
        if result is None:
            self.log.error("✗ Synthesisability: FAIL")
            self.log.error("Time exceeded")
            synthesisability["status"] = False
            synthesisability["error"]["types"] = ["Time Exceeded"]
            synthesisability["error"]["count"] = None
            synthesisability["warning"]["types"] = None
            synthesisability["warning"]["count"] = None
            return synthesisability  # Early return

        synthesisability["status"] = result.returncode == 0
        if synthesisability["status"]:
            self.log.info("✓ Synthesisability: PASS")
        else:
            self.log.info("✗ Synthesisability: FAIL")
        if result.stderr:
            self.log.warning("⚠ Warning/Error found")
            std_errors = result.stderr.splitlines()
            # Check for errors and warnings:
            errors, warnings = self._group_errors_and_warnings(std_errors)
            synthesisability["error"]["types"] = errors
            synthesisability["error"]["count"] = None if errors is None else len(errors)
            synthesisability["warning"]["types"] = warnings
            synthesisability["warning"]["count"] = (
                None if warnings is None else len(warnings)
            )
        else:
            synthesisability["error"]["types"] = None
            synthesisability["error"]["count"] = None
            synthesisability["warning"]["types"] = None
            synthesisability["warning"]["count"] = None

        return synthesisability
