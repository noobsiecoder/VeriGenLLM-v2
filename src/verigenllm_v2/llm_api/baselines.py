"""
Main Python file for benching Verilog code generated by proprietary LLMs

Note: Gemini API calls are commented since the usage was erratic. (even though the API's usage tier was: Tier 1)

This script:
1. Loads Verilog problem prompts from the test dataset
2. Sends prompts to multiple commercial LLMs (Claude, OpenAI, Gemini)
3. Collects multiple responses per prompt
4. Saves responses for later evaluation

Author: Abhishek Sriram <noobsiecoder@gmail.com>
Date:   Jul 31st, 2025

Note: LLM was used to generate comments
"""

import os
import json
import random
from dotenv import load_dotenv
from verigenllm_v2.utils.logger import Logger
from claude_client import ClaudeClient, ClaudeModels

# Gemini client commented out due to API reliability issues
# from gemini_client import GeminiClient, GeminiModels
from openai_client import OpenAIClient, OpenAIModels

if __name__ == "__main__":
    # Initialize logger for tracking script execution
    log = Logger("baseline_client").get_logger()

    # Load API keys from environment file
    # This file should contain: CLAUDE_API, OPENAI_API, GEMINI_API
    load_dotenv("secrets/models-api.env")
    log.info("Loaded API key from ENV file")

    # Helper function to construct dataset paths
    # Lambda function makes it easy to create paths for different models
    dataset_path = (
        lambda path: f"dataset/models/{path}"
    )  # e.g., dataset_path("claude") -> "dataset/models/claude"

    # Path to test problems directory
    test_dataset_path = "dataset/test/"  # Contains 18 problem folders

    # Initialize list to store all prompts
    prompts = []  # Will contain one prompt per problem

    # Get all directories in test folder
    # Each directory contains one problem with metadata.json
    dirs = os.listdir(test_dataset_path)
    dirs.sort()  # Ensure consistent ordering (example_01, example_02, ...)

    # Process each problem directory
    for dir in dirs:
        # Skip macOS metadata file
        if dir == ".DS_Store":
            continue
        else:
            # Each problem directory has a metadata.json file
            metadata_filepath = os.path.join(test_dataset_path, dir, "metadata.json")

            # Load metadata containing multiple prompt variations
            with open(metadata_filepath, "r") as fs:
                metadata = json.load(fs)

                # Randomly select one prompt variation for this problem
                # This adds diversity to the evaluation
                prompt = random.choice(metadata["prompts"])
                prompts.append(prompt)

    # Verify prompts were loaded successfully
    if len(prompts) != 0:
        log.info(f"All prompts loaded - size: {len(prompts)}")
    else:
        log.warning("Prompts not loaded")

    # Initialize API clients for each LLM provider
    # Each client handles API communication and response formatting
    clients = {
        "claude": ClaudeClient(
            api_key=os.environ["CLAUDE_API"],
            model=ClaudeModels.Opus_4.value,  # Using Claude Opus 4 model
        ),
        # Gemini client disabled due to unreliable API behavior
        # Despite being on Tier 1, the API had erratic performance
        # "gemini": GeminiClient(
        #     api_key=os.environ["GEMINI_API"],
        #     model=GeminiModels.GEMINI_2_5_PRO.value,
        # ),
        "openai": OpenAIClient(
            api_key=os.environ["OPENAI_API"],
            model=OpenAIModels.GPT_4_1.value,  # Using GPT-4.1 model
        ),
    }

    # Initialize response storage for each model
    claude_responses = []
    # gemini_responses = []  # Disabled
    openai_responses = []

    # Process prompts - currently only processing problems 15-17 (indices 15:18)
    # This is likely for testing or continuing from a previous run
    # Change to prompts[:] to process all problems
    for prompt in prompts[15:18]:
        # Generate responses from each model
        # n_samples=10 means each model generates 10 different solutions per prompt
        response = {
            "claude": clients["claude"].generate(prompt, n_samples=10),
            # "gemini": clients["gemini"].generate(prompt, n_samples=6),  # Fewer samples due to API limits
            "openai": clients["openai"].generate(prompt, n_samples=10),
        }

        # Collect responses from each model
        claude_responses.append(response["claude"])
        # gemini_responses.append(response["gemini"])
        openai_responses.append(response["openai"])

    # Save Claude responses to JSON file
    # Using "+a" mode which creates file if not exists, appends if exists
    with open(
        os.path.join(dataset_path("claude"), "claude-response-n10.json"), "+a"
    ) as fs:
        # Save with formatting for readability
        json.dump(claude_responses, fs, indent=4, ensure_ascii=False)
        log.info(
            f"Dumped data into: {os.path.join(dataset_path('claude'), 'claude-response-n10.json')}"
        )

    # Gemini response saving disabled
    # with open(
    #     os.path.join(dataset_path("gemini"), "gemini-response-n6.json"), "+a"
    # ) as fs:
    #     json.dump(gemini_responses, fs, indent=4, ensure_ascii=False)
    #     log.info(
    #         f"Dumped data into: {os.path.join(dataset_path('gemini'), 'gemini-response-n10.json')}"
    #     )

    # Save OpenAI responses to JSON file
    with open(
        os.path.join(dataset_path("openai"), "openai-response-n10.json"), "+a"
    ) as fs:
        json.dump(openai_responses, fs, indent=4, ensure_ascii=False)
        log.info(
            f"Dumped data into: {os.path.join(dataset_path('openai'), 'openai-response-n10.json')}"
        )
